# 🚀 **Machine Translation using Seq2Seq Model with Attention Mechanism** 🌐

I’m excited to share my recent project where I built a **sequence-to-sequence (Seq2Seq)** model with an **Attention Mechanism** for **English-to-French translation**. This project allowed me to dive deep into **natural language processing (NLP)** and **sequence modeling**. 🧠📊

### Key Highlights:
- 🔹 **Data Cleaning**: Applied text preprocessing techniques, including handling contractions and non-alphabetical characters.
- 🔹 **Tokenizer and Padding**: Tokenized and padded the English and French sentences for model input.
- 🔹 **Bidirectional LSTM & Attention**: Used a **Bidirectional LSTM** in the encoder and implemented an **Attention Mechanism** to improve the translation quality.
- 🔹 **Training and Evaluation**: Trained the model on a large dataset with early stopping and evaluated its performance using **accuracy** and **loss** metrics.
- 🔹 **Inference Model**: Created an inference model to translate new sentences by leveraging the trained encoder-decoder architecture.

### Results:
- ✔️ Achieved 90% **translation accuracy**.
- ✔️ Gained insights into **NLP tasks** like **machine translation** and **attention mechanisms**.

You can check out the full notebook and implementation on [Kaggle](https://www.kaggle.com/code/hamoi9/language-translation-english-french-usingseq2seq#Evaluate-and-Prediciton).
