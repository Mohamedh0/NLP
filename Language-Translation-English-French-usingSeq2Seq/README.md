# ğŸš€ **Machine Translation using Seq2Seq Model with Attention Mechanism** ğŸŒ

Iâ€™m excited to share my recent project where I built a **sequence-to-sequence (Seq2Seq)** model with an **Attention Mechanism** for **English-to-French translation**. This project allowed me to dive deep into **natural language processing (NLP)** and **sequence modeling**. ğŸ§ ğŸ“Š

### Key Highlights:
- ğŸ”¹ **Data Cleaning**: Applied text preprocessing techniques, including handling contractions and non-alphabetical characters.
- ğŸ”¹ **Tokenizer and Padding**: Tokenized and padded the English and French sentences for model input.
- ğŸ”¹ **Bidirectional LSTM & Attention**: Used a **Bidirectional LSTM** in the encoder and implemented an **Attention Mechanism** to improve the translation quality.
- ğŸ”¹ **Training and Evaluation**: Trained the model on a large dataset with early stopping and evaluated its performance using **accuracy** and **loss** metrics.
- ğŸ”¹ **Inference Model**: Created an inference model to translate new sentences by leveraging the trained encoder-decoder architecture.

### Results:
- âœ”ï¸ Achieved 90% **translation accuracy**.
- âœ”ï¸ Gained insights into **NLP tasks** like **machine translation** and **attention mechanisms**.

You can check out the full notebook and implementation on [Kaggle](https://www.kaggle.com/code/hamoi9/language-translation-english-french-usingseq2seq#Evaluate-and-Prediciton).
