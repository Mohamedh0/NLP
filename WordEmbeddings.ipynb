{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamedh0/NLP/blob/main/WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revisiting BOW, TF-IDF"
      ],
      "metadata": {
        "id": "2pBsebGFzP6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 1: Recap on BOW and TF-IDF**\n",
        "\n",
        "In this section, we will simply remind ourselves of the Bag of Words (BOW) and TF-IDF, as the focus will shift to word embeddings later.\n",
        "\n",
        "\n",
        "- **BOW** is a simple representation where each word in the document is represented by its frequency in the document.\n",
        "- It disregards grammar and word order but keeps track of word occurrences.\n",
        "\n",
        "#### 1.2 **TF-IDF**\n",
        "- **TF-IDF** stands for \"Term Frequency-Inverse Document Frequency\".\n",
        "- It weighs words by how important they are in a document, considering the term frequency (TF) and how rare a term is in the entire corpus (IDF).\n"
      ],
      "metadata": {
        "id": "-060WfpdzIGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"The dog barks\", \"The dog runs fast\", \"The cat sleeps\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "M6uwTB9gwEEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "aeROdQLswHWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-Embeddings"
      ],
      "metadata": {
        "id": "xmOLIuWlz03k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 **What are Word Embeddings?**\n",
        "- Word embeddings are dense vector representations of words where similar words have similar vector representations.\n",
        "- Unlike traditional methods like **BOW** or **TF-IDF**, word embeddings capture semantic meaning and relationships between words, such as synonyms, antonyms, and even analogies.\n",
        "- Embeddings are learned from context, not just word frequency or importance.\n",
        "\n",
        "#### 2.2 **Why Are Word Embeddings Important?**\n",
        "- Word embeddings address the limitation of BOW and TF-IDF where word order and meaning are lost.\n",
        "- They allow models to understand semantic relationships between words. For instance, the vectors for **\"king\"** and **\"queen\"** would be closer than \"king\" and \"dog\".\n",
        "\n",
        "#### 2.3 **Types of Word Embeddings**\n",
        "Some of the famous word embedding algorithms are:\n",
        "- **Word2Vec**: Learns embeddings based on the context words of a target word (Skip-Gram and CBOW).\n",
        "- **GloVe**: Uses matrix factorization techniques to find word embeddings based on word co-occurrence.\n",
        "- **FastText**: An extension of Word2Vec that represents words as bags of character n-grams.\n",
        "\n",
        "We will focus on **Word2Vec**, a widely used algorithm, in the next steps.\n",
        "\n",
        "#### 2.4 **Skip-Gram and CBOW (Continuous Bag of Words)**\n",
        "\n",
        "- **Skip-Gram**: Predicts the surrounding context words (context) given a target word.\n",
        "  - E.g., Given \"dog\" as the target, predict words like \"barks\", \"chases\", etc.\n",
        "  \n",
        "- **CBOW**: Predicts the target word based on the surrounding context.\n",
        "  - E.g., Given words like \"barks\", \"chases\", predict the target word \"dog\".\n",
        "\n",
        "In the next part, we will start building a **Word2Vec model** using a neural network."
      ],
      "metadata": {
        "id": "6VN5Yq4Qz3Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building a Word2Vec Model using CBOW**\n",
        "\n",
        "#### 3.1 **Overview of CBOW**\n",
        "In the **CBOW** (Continuous Bag of Words) model, we are tasked with predicting the target word based on a given context. The context consists of a fixed-size window of surrounding words.\n",
        "\n",
        "For example:\n",
        "- Given the context words: \"the\", \"dog\", \"chases\"\n",
        "- The model tries to predict the target word: \"cat\"\n",
        "\n",
        "#### 3.2 **Steps to Build the CBOW Model**\n",
        "We will implement CBOW from scratch using a neural network with the following steps:\n",
        "1. **Data Preparation**: Convert the text into pairs of context-target words.\n",
        "2. **Create Vocabulary**: Map words to unique integers (word index).\n",
        "3. **Input Layer**: The context words will be one-hot encoded.\n",
        "4. **Hidden Layer**: The embeddings will be learned in this layer.\n",
        "5. **Output Layer**: Use softmax to predict the target word.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3 **Implementing CBOW in Code**\n",
        "\n",
        "We'll need to install some libraries and use a sample corpus to build this model.\n",
        "\n",
        "#### 3.4 **What Happens Here:**\n",
        "1. **Tokenizer**: We convert the words into sequences of integers (a word index).\n",
        "2. **Context-Target Generation**: For each word in a sequence, we generate a context of surrounding words, and the target is the central word.\n",
        "3. **Model Architecture**:\n",
        "   - **Embedding Layer**: This layer learns the word embeddings. The input size is the vocabulary size, and the output size is the embedding dimension (50 in this case).\n",
        "   - **Flatten Layer**: To make the output suitable for a Dense layer.\n",
        "   - **Dense Layer**: This gives the output probabilities for each word in the vocabulary.\n",
        "4. **Training**: The model is trained using the context-target pairs.\n",
        "\n",
        "The embeddings will be learned during the training process. After training, you can extract the word embeddings from the **embedding layer** of the model.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7s1J5K0K0Adf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"the dog barks\",\n",
        "    \"the dog chases the cat\",\n",
        "    \"the cat sleeps\",\n",
        "    \"the dog runs fast\"\n",
        "]\n",
        "\n",
        "# 1. Data Preparation: Create a context-target pair (CBOW)\n",
        "window_size = 2  # Context size is 2, meaning 2 words before and 2 after the target\n",
        "\n",
        "# Tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding (if needed)\n",
        "\n",
        "# Convert words to integer tokens\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "# Generate context-target pairs (X is context, y is target)\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        context = seq[i-window_size:i] + seq[i+1:i+window_size+1]  # Context words\n",
        "        target = seq[i]  # Target word\n",
        "        X.append(context)\n",
        "        y.append(target)\n",
        "\n",
        "# Pad the context words to ensure uniform length\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# 2. Create the CBOW Model\n",
        "embedding_dim = 50  # Size of the embedding vector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=window_size * 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the Model\n",
        "model.summary()\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Now the embeddings are learned!"
      ],
      "metadata": {
        "id": "L0CdS4jov3va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-Gram, N-Skip-Gram"
      ],
      "metadata": {
        "id": "BHZPTiu11C7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip-Gram model implementation\n",
        "\n",
        "# Context size (window size)\n",
        "window_size = 2\n",
        "\n",
        "# Generate context-target pairs for Skip-Gram\n",
        "X_skipgram = []\n",
        "y_skipgram = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        target = seq[i]  # Target word\n",
        "        context = seq[i-window_size:i] + seq[i+1:i+window_size+1]  # Context words\n",
        "        for word in context:\n",
        "            X_skipgram.append([target])\n",
        "            y_skipgram.append(word)\n",
        "\n",
        "X_skipgram = np.array(X_skipgram)\n",
        "y_skipgram = np.array(y_skipgram)\n",
        "\n",
        "# Build Skip-Gram model\n",
        "model_skipgram = Sequential()\n",
        "model_skipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
        "model_skipgram.add(Flatten())\n",
        "model_skipgram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_skipgram.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Skip-Gram model\n",
        "model_skipgram.summary()\n",
        "model_skipgram.fit(X_skipgram, y_skipgram, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "dF4VST0Yur7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 **Overview of Skip-Gram**\n",
        "In the **Skip-Gram** model, the goal is to predict the context words given a target word. The target word is the central word in the context window, and the model learns to predict the surrounding words.\n",
        "\n",
        "For example:\n",
        "- Given the target word: \"dog\"\n",
        "- The model tries to predict words like \"barks\", \"chases\", etc.\n",
        "\n",
        "This is the reverse of the **CBOW** approach, where context predicts the target.\n",
        "\n",
        "#### 4.2 **Skip-Gram Implementation**\n",
        "\n",
        "We'll start by implementing the Skip-Gram model, similar to the previous CBOW approach, but the difference is that the target is the central word and the context words are predicted.\n",
        "\n",
        "\n",
        "#### 4.3 **What Happens in Skip-Gram:**\n",
        "- **Input**: The input is a single target word (central word) in the context window.\n",
        "- **Output**: The output is the probability distribution of context words. The model tries to maximize the probability of predicting the context words given the target word.\n",
        "\n",
        "The architecture here is quite similar to the CBOW, with the main difference being that Skip-Gram predicts multiple context words for a single target word.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sfvV1r5s1Jfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N-Skip-Gram implementation\n",
        "N = 3  # We can vary N to predict more context words\n",
        "\n",
        "X_nskipgram = []\n",
        "y_nskipgram = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        target = seq[i]\n",
        "        context = seq[i-N:i] + seq[i+1:i+N+1]  # Larger context window (N words)\n",
        "        for word in context:\n",
        "            X_nskipgram.append([target])\n",
        "            y_nskipgram.append(word)\n",
        "\n",
        "X_nskipgram = np.array(X_nskipgram)\n",
        "y_nskipgram = np.array(y_nskipgram)\n",
        "\n",
        "# Build N-Skip-Gram model\n",
        "model_nskipgram = Sequential()\n",
        "model_nskipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
        "model_nskipgram.add(Flatten())\n",
        "model_nskipgram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_nskipgram.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the N-Skip-Gram model\n",
        "model_nskipgram.summary()\n",
        "model_nskipgram.fit(X_nskipgram, y_nskipgram, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "C8yca6WavQCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 5: N-Skip-Gram Model**\n",
        "\n",
        "The **N-Skip-Gram** model is an extension of Skip-Gram, where the target word is used to predict the **N** context words, but instead of predicting each context word individually, the model tries to predict a larger set of context words (possibly with larger context windows).\n",
        "\n",
        "#### 5.1 **Difference with Skip-Gram**:\n",
        "- **Skip-Gram**: Predicts a set of context words for each target word. The model output is a probability distribution over the vocabulary for each context word.\n",
        "- **N-Skip-Gram**: Instead of predicting a fixed number of context words (as in Skip-Gram), we predict **N** context words over a larger window of words around the target word.\n",
        "\n",
        "Here’s how we can implement the **N-Skip-Gram** model:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 6: Difference Between Skip-Gram and N-Skip-Gram**\n",
        "\n",
        "- **Skip-Gram**: For each target word, predicts a fixed number of context words (one context word at a time). The size of the context window is usually fixed.\n",
        "- **N-Skip-Gram**: Expands on the Skip-Gram model by predicting multiple context words (based on a larger context window). The difference is that N-Skip-Gram tries to capture a broader context for each target word by considering more context words in its predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pXkFux0IvYy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Trained Embeddings"
      ],
      "metadata": {
        "id": "pbeWscC_1Vql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 5: Pre-trained Word Embeddings**\n",
        "\n",
        "#### 7.1 **What Are Pre-trained Word Embeddings?**\n",
        "\n",
        "Pre-trained word embeddings are vector representations of words that have been learned from large corpora of text and can be used directly in downstream tasks like text classification, sentiment analysis, etc.\n",
        "\n",
        "The benefit of pre-trained embeddings is that they capture rich semantic relationships between words, which can significantly improve performance in many NLP tasks compared to training embeddings from scratch.\n",
        "\n",
        "#### 7.2 **Types of Pre-trained Word Embeddings**\n",
        "\n",
        "1. **Word2Vec (Skip-Gram and CBOW)**:\n",
        "   - Word2Vec is an unsupervised learning algorithm used to learn word embeddings from a large corpus.\n",
        "   - The two models in Word2Vec are **Skip-Gram** and **CBOW** (which we discussed earlier).\n",
        "\n",
        "   The **Skip-Gram model** maximizes the following objective function:\n",
        "   \\[\n",
        "   J(\\theta) = -\\sum_{t=1}^{T} \\sum_{-C \\leq j \\leq C, j \\neq 0} \\log p(w_{t+j} | w_t)\n",
        "   \\]\n",
        "   - **Target Word**: \\(w_t\\)\n",
        "   - **Context Words**: \\(w_{t+j}\\) (within the context window \\(C\\))\n",
        "   - **Probability**: \\(p(w_{t+j} | w_t)\\) is the conditional probability of observing the context word given the target word.\n",
        "   \n",
        "   For the **CBOW** model, the goal is to predict the target word based on the context words. The equation is:\n",
        "   \\[\n",
        "   J(\\theta) = -\\sum_{t=1}^{T} \\log p(w_t | context_{t})\n",
        "   \\]\n",
        "   where the context is the surrounding words and the target is the center word.\n",
        "\n",
        "   - **Objective**: The models attempt to maximize the likelihood of the context given the target (Skip-Gram) or maximize the likelihood of the target given the context (CBOW).\n",
        "\n",
        "---\n",
        "\n",
        "2. **GloVe (Global Vectors for Word Representation)**:\n",
        "   - GloVe is a word embedding model based on **matrix factorization**. It uses the global word co-occurrence statistics of a corpus.\n",
        "   - The objective of GloVe is to factorize the word co-occurrence matrix \\(X\\) (where each element \\(X_{ij}\\) represents the number of times word \\(i\\) appears in the context of word \\(j\\)).\n",
        "   \n",
        "   The equation for GloVe is:\n",
        "   \\[\n",
        "   J(\\theta) = \\sum_{i=1}^{V} \\sum_{j=1}^{V} f(X_{ij}) \\left( w_i^T w_j + b_i + b_j - \\log X_{ij} \\right)^2\n",
        "   \\]\n",
        "   - **Objective**: The objective is to minimize the squared error between the actual co-occurrence (\\(\\log X_{ij}\\)) and the predicted co-occurrence (\\(w_i^T w_j + b_i + b_j\\)), with \\(f(X_{ij})\\) as a weighting function to balance the influence of rare and frequent co-occurrences.\n",
        "   - **\\(w_i\\)** and **\\(w_j\\)**: These are the embedding vectors for words \\(i\\) and \\(j\\).\n",
        "   - **\\(b_i\\)** and **\\(b_j\\)**: Bias terms for the words.\n",
        "\n",
        "   GloVe attempts to learn embeddings such that the dot product between two word vectors \\(w_i\\) and \\(w_j\\) is close to the log of the co-occurrence count.\n",
        "\n",
        "---\n",
        "\n",
        "3. **FastText**:\n",
        "   - FastText is an extension of Word2Vec where each word is represented as a bag of character n-grams. This allows the model to generate embeddings for **out-of-vocabulary words** (words not seen during training).\n",
        "   \n",
        "   The main difference with Word2Vec is that it uses subword information, so for a word \\(w\\), FastText breaks it into subwords (e.g., n-grams of characters).\n",
        "   - For example, the word “apple” can be represented as a combination of n-grams like **‘ap’, ‘pp’, ‘pl’, ‘le’** (depending on the n-gram size chosen).\n",
        "\n",
        "   The embedding for a word is a sum of the embeddings of all its subword n-grams.\n",
        "\n",
        "   The equation for FastText would be similar to Word2Vec but considering n-grams:\n",
        "   \\[\n",
        "   \\text{Emb}(w) = \\sum_{\\text{ngram}(w)} \\text{Emb}(\\text{ngram})\n",
        "   \\]\n",
        "   where the **ngram(w)** represents all character n-grams derived from the word \\(w\\).\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.3 **How to Use Pre-trained Word Embeddings**\n",
        "\n",
        "Using pre-trained embeddings involves loading an existing model that has been trained on a large corpus, such as **Google's Word2Vec**, **GloVe**, or **FastText**, and using these vectors for tasks like similarity measurement, text classification, etc.\n",
        "\n",
        "Here’s an example of how to load pre-trained **GloVe** embeddings and use them with a Keras model:"
      ],
      "metadata": {
        "id": "TlKNiZlR1X72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "QvnNhUuLzOpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "MdMqXE8Dudr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe pre-trained embeddings\n",
        "import numpy as np\n",
        "\n",
        "# Define the embedding dimension and file path for GloVe (e.g., 100-dimensional vectors)\n",
        "embedding_dim = 100\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "# Load the GloVe word vectors into a dictionary\n",
        "embeddings_index = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# Example: Access the embedding for the word 'king'\n",
        "embedding_king = embeddings_index['king']\n",
        "print(\"Embedding for 'king':\", embedding_king)"
      ],
      "metadata": {
        "id": "HM9EbJSPuYeR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}